Final Project Explanation for CS2550

Running:
The commands to run the myPTM system are as follows:

myPTM --scheduler <rr|seed> --buffer <page #> --processes <files>

For example to run the transactions in A.txt and B.txt using random scheduling with a seed of 2550 and a buffer size of 10 the command would be:

myPTM --scheduler 2550 --buffer 10 --processes A.txt B.txt

Note that the "rr" option is for round robin scheduling.

Concurreny Control:

We implemented the Wound-Wait deadlock avoidance scheme. However, something we missed in our design was the different lock compatibilities for transactions verse processes. This was not a difficult adjustment to make. Another minor deviation from our initial design was the way we implmented the lock tree. In our meeting with the TA we realized we would only need to either lock data files for deleting them, or records for inserting or updating. Thus, our locking scheme is based around file and record locks. The logical structure of our locking has a tree of files which contains trees of RecordLocks. The record lock represents the level of an element and therefore only one record lock exists for each element. Within the record lock are two lists of Lock Types.  These represent the actual lock type (transaction read, process write, etc.) one for the transactions granted locks to the element, and one for the transaction which have been queued and are waiting to get the lock. Another little oddity is that to logically fit even though we called the top level a (file) lock tree, the implementation was made easier by storing the flag for the file lock in the record lock tree.  We also attempted to make the checking for whether a lock could be granted and releasing locks quicker by storing references in both the tree (to the transactions which hold the lock) and references in the transactions (to the locks which the transaction holds) thereby reducing the amound of tree/list traversal. Of course this required a bit more bookkeeping to keep it all straight.

We also left the checkpointing behind as it was not needed for this system. The simplified log buffer only tracks writes and deletes. The log entries contain field for data file id, transaction id, page id, before image and after image. So on every write and delete a new entry was added to the log buffer. Then on commits the log buffer would be scanned and those entries tied to that transaction are removed and the backed up data files are destroyed. On abort a list of entries will be compiled, in order. Then the Data Manager will restore and remove records and data files back to a state before the aborted transaction started.

Other Interesting Features:
**Any thoughts here Jeff??**

